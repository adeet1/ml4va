# -*- coding: utf-8 -*-
"""ML4VA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jAec49ZyqvufMxEHIObLlOvC7gHjF5n3
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from pandas.plotting import register_matplotlib_converters
import os 
import datetime
import IPython
import IPython.display
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression

np.random.seed(0)

from google.colab import drive
drive.mount('/content/drive')
filepath = "drive/My Drive/Documents/Education/University - 1 - UVA/Coursework - 6 - 2020 Fall/CS 4774 Machine Learning/ML4VA Project/ml4va"

# Import data
df0 = pd.read_csv(filepath + "/Virginia_Crashes.csv")
df0.head()

# Shape of original dataset
df0.shape

#Generate interactive geographic plot of Crashes in Virginia
import plotly.express as px
df20 = df0[(df0.CRASH_YEAR == 2019) | (df0.CRASH_YEAR == 2020)]

fig = px.scatter_mapbox(df20, lat='LATITUDE', lon='LONGITUDE', 
                        color='Crash_Severity', zoom=5.5,
                        hover_name='VDOT_District',hover_data=['Weather_Condition','Collision_Type','Rte_Nm']
                       )
fig.update_layout(mapbox_style="open-street-map")
fig.show()

# Drop redundant columns
df = df0.drop(["OBJECTID", "Document_Nbr", "Rte_Nm", "Local_Case_Cd", "DIAGRAM", "Node_Info", "X", "Y"], axis = 1)

# Filter the dataset to only contain crashes from the past two years
indices = np.where(np.logical_or(df["CRASH_YEAR"] == 2019, df["CRASH_YEAR"] == 2020))[0]
df_filt = df.iloc[indices, :].reset_index(drop = True)

# Compare shapes of original and filtered dataset
print(df.shape, "-->", df_filt.shape)

# Commented out IPython magic to ensure Python compatibility.
#Distributions of dataset features
# %matplotlib inline
import matplotlib.pyplot as plt
df_filt.hist(bins=12, figsize=(15, 10))
plt.show()

# Distribution of classes
dist = df_filt["Crash_Severity"].value_counts(ascending = False) / df_filt["Crash_Severity"].size
pd.DataFrame(dist)

plt.pie(dist, labels = dist.index, radius = 2)
plt.show()

#Scatter matrix of particular features in data
from pandas.plotting import scatter_matrix

attributes = ["Pedage", "Passage", "LATITUDE",
              "LONGITUDE", "VSP"]
scatter_matrix(df_filt[attributes], figsize=(12, 8))
plt.show()

# A dictionary containing variable names by type
variables = {
        "ordinal" : ["Time_Slicing", "Speed_Notspeed", "Belted_Unbelted", "Alcohol_Notalcohol"],
        "nominal" : ["Weather_Condition", "First_Harmful_Event_of_Entire_C", "Collision_Type", "FAC", "FUN", "Light_Condition", "VDOT_District", "Ownership_Used", "Crash_Event_Type_Dsc", "Roadway_Surface_Cond"],
        "numerical": ["Rns_Mp", "K_People", "A_People", "B_People", "C_People", "LATITUDE", "LONGITUDE", "VSP", "SYSTEM", "OWNERSHIP", "Carspeedlimit", "Crash_Military_Tm"],
        "target": ["Crash_Severity"]
        }

# Split into X and Y
X = df_filt[variables["ordinal"] + variables["nominal"] + variables["numerical"]]
Y = df_filt["Crash_Severity"]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)

# Reset index
X_train = X_train.reset_index(drop = True)
X_test = X_test.reset_index(drop = True)
Y_train = Y_train.reset_index(drop = True)
Y_test = Y_test.reset_index(drop = True)

print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)

pd.Series(Y_train).value_counts().sort_values(ascending = False) / Y_train.size

pd.Series(Y_test).value_counts().sort_values(ascending = False) / Y_test.size

# Split into ordinal and numerical sets
X_train_ordinal = X_train[variables["ordinal"]]
X_train_numerical = X_train[variables["numerical"]]
X_test_ordinal = X_test[variables["ordinal"]]
X_test_numerical = X_test[variables["numerical"]]

# Remove any rows with missing values for nominal features
X_train_nominal = X_train[variables["nominal"]]
X_train_nominal_missing = np.array(X_train_nominal.isna()).any(axis = 1)
X_train_nominal_missing_indices = np.where(X_train_nominal_missing)[0]
X_train.drop(X_train_nominal_missing_indices, inplace = True)
X_train_ordinal.drop(X_train_nominal_missing_indices, inplace = True)
X_train_numerical.drop(X_train_nominal_missing_indices, inplace = True)
X_train_nominal.drop(X_train_nominal_missing_indices, inplace = True)
Y_train.drop(X_train_nominal_missing_indices, inplace = True)

X_test_nominal = X_test[variables["nominal"]]
X_test_nominal_missing = np.array(X_test_nominal.isna()).any(axis = 1)
X_test_nominal_missing_indices = np.where(X_test_nominal_missing)[0]

X_test.drop(X_test_nominal_missing_indices, inplace = True)
X_test_ordinal.drop(X_test_nominal_missing_indices, inplace = True)
X_test_numerical.drop(X_test_nominal_missing_indices, inplace = True)
X_test_nominal.drop(X_test_nominal_missing_indices, inplace = True)
Y_test.drop(X_test_nominal_missing_indices, inplace = True)

# Combine ordinal, numerical, and nominal sets
X_train = pd.concat((X_train_numerical, X_train_ordinal, X_train_nominal), axis = 1)
X_test = pd.concat((X_test_numerical, X_test_ordinal, X_test_nominal), axis = 1)

# Build data transformation pipeline
steps_ordinal = [("encoder", OrdinalEncoder())]
steps_nominal = [("encoder", OneHotEncoder(drop = "first"))]
steps_numerical = [("imputer", SimpleImputer(strategy = "median")), \
                   ("scaler", StandardScaler())]
pipeline_ordinal = Pipeline(steps_ordinal)
pipeline_nominal = Pipeline(steps_nominal)
pipeline_numerical = Pipeline(steps_numerical)

steps_all = [("ordinal", pipeline_ordinal, variables["ordinal"]),
             ("nominal", pipeline_nominal, variables["nominal"]),
             ("numerical", pipeline_numerical, variables["numerical"])]
pipeline = ColumnTransformer(steps_all)

X_train_tr = pipeline.fit_transform(X_train).toarray()
X_test_tr = pipeline.transform(X_test).toarray()

# Label encoding
label_map = {"PDO.Property Damage Only" : 0,
             "C.Nonvisible Injury"      : 1,
             "B.Visible Injury"         : 2,
             "A.Severe Injury"          : 3,
             "K.Fatal Injury"           : 4}
Y_train_tr = Y_train.replace(label_map)
Y_test_tr = Y_test.replace(label_map)

# Compare array shapes before and after encoding
print("X_train:", X_train.shape, "-->", X_train_tr.shape)
print("Y_train:", Y_train.shape)
print("X_test:", X_test.shape, "-->", X_test_tr.shape)
print("Y_test:", Y_test.shape)

pd.Series(Y_train_tr).value_counts().sort_values(ascending = False) / Y_train_tr.size

pd.Series(Y_test_tr).value_counts().sort_values(ascending = False) / Y_test_tr.size

# Visualize PCA for various numbers of principal components
pca = PCA()
pca.fit(X_train_tr)
pca_variances = np.cumsum(pca.explained_variance_ratio_)
plt.figure()
plt.plot(pca_variances)
plt.xlabel('Number of Components')
plt.ylabel('Variance (%)') # for each component
plt.show()

# Apply PCA to the data
n_pc = np.where(pca_variances >= 0.99)[0][0] + 1
pca = PCA(n_components = n_pc).fit(X_train_tr)
X_train_pca = pca.transform(X_train_tr)
X_test_pca = pca.transform(X_test_tr)

# Compare array shapes before and after PCA
print("X_train:", X_train_tr.shape, "-->", X_train_pca.shape)
print("Y_train:", Y_train_tr.shape)
print("X_test:", X_test_tr.shape, "-->", X_test_pca.shape)
print("Y_test:", Y_test_tr.shape)

# Number of classes
n_classes = np.unique(Y_train_tr).size
n_classes

model = LogisticRegression()
model.fit(X_train_pca, Y_train_tr)

# Make predictions
Y_train_pred = model.predict(X_train_pca)
Y_test_pred = model.predict(X_test_pca)

# Confusion matrix for training set
pd.DataFrame(confusion_matrix(Y_train_pred, Y_train_tr))

# Confusion matrix for testing set
pd.DataFrame(confusion_matrix(Y_test_pred, Y_test_tr))

# Calculate metrics for training set
acc_train = accuracy_score(Y_train_tr, Y_train_pred)
prec_train = precision_score(Y_train_tr, Y_train_pred, average = None)
recall_train = recall_score(Y_train_tr, Y_train_pred, average = None)
f1_train = f1_score(Y_train_tr, Y_train_pred, average = None)

# Calculate metrics for testing set
acc_test = accuracy_score(Y_test_tr, Y_test_pred)
prec_test = precision_score(Y_test_tr, Y_test_pred, average = None)
recall_test = recall_score(Y_test_tr, Y_test_pred, average = None)
f1_test = f1_score(Y_test_tr, Y_test_pred, average = None)

# Create dataframe
metrics_train = np.array([acc_train, \
                          prec_train[0], prec_train[1], prec_train[2], prec_train[3], prec_train[4], \
                          recall_train[0], recall_train[1], recall_train[2], recall_train[3], recall_train[4], \
                          f1_train[0], f1_train[1], f1_train[2], f1_train[3], f1_train[4]])

metrics_test = np.array([acc_test, \
                         prec_test[0], prec_test[1], prec_test[2], prec_test[3], prec_test[4], \
                         recall_test[0], recall_test[1], recall_test[2], recall_test[3], recall_test[4], \
                         f1_test[0], f1_test[1], f1_test[2], f1_test[3], f1_test[4]])

metrics = {"Training Set": metrics_train, "Testing Set": metrics_test}
metrics = pd.DataFrame(metrics)
metrics.index = ["Accuracy", \
                 "Precision (Class 0)", "Precision (Class 1)", "Precision (Class 2)", "Precision (Class 3)", "Precision (Class 4)", \
                 "Recall (Class 0)", "Recall (Class 1)", "Recall (Class 2)", "Recall (Class 3)", "Recall (Class 4)", \
                 "F1 Score (Class 0)", "F1 Score (Class 1)", "F1 Score (Class 2)", "F1 Score (Class 3)", "F1 Score (Class 4)"]

# Display metrics
metrics