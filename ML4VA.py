# -*- coding: utf-8 -*-
"""ML4VA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jAec49ZyqvufMxEHIObLlOvC7gHjF5n3
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from pandas.plotting import register_matplotlib_converters
import os 
import datetime
import IPython
import IPython.display
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression

np.random.seed(0)

# Import data
df0 = pd.read_csv("Virginia_Crashes.csv")
df0.head()

# Shape of original dataset
df0.shape

# Drop redundant columns
df = df0.drop(["OBJECTID", "Document_Nbr", "Rte_Nm", "Local_Case_Cd", "DIAGRAM", "Node_Info", "X", "Y"], axis = 1)

# Filter the dataset to only contain crashes from the past two years
indices = np.where(np.logical_or(df["CRASH_YEAR"] == 2019, df["CRASH_YEAR"] == 2020))[0]
df_filt = df.iloc[indices, :].reset_index(drop = True)

# A dictionary containing variable names by type
variables = {
        "ordinal" : ["Time_Slicing", "Speed_Notspeed", "Belted_Unbelted", "Alcohol_Notalcohol"],
        "nominal" : ["Weather_Condition", "First_Harmful_Event_of_Entire_C", "Collision_Type", "FAC", "FUN", "Light_Condition", "VDOT_District", "Ownership_Used", "Crash_Event_Type_Dsc", "Roadway_Surface_Cond"],
        "numerical": ["Rns_Mp", "K_People", "A_People", "B_People", "C_People", "LATITUDE", "LONGITUDE", "VSP", "SYSTEM", "OWNERSHIP", "Carspeedlimit", "Crash_Military_Tm"],
        "target": ["Crash_Severity"]
        }

# Split into X and Y
X = df_filt[variables["ordinal"] + variables["nominal"] + variables["numerical"]]
Y = df_filt["Crash_Severity"]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)

# Reset index
X_train = X_train.reset_index(drop = True)
X_test = X_test.reset_index(drop = True)
Y_train = Y_train.reset_index(drop = True)
Y_test = Y_test.reset_index(drop = True)

# Split into ordinal and numerical sets
X_train_ordinal = X_train[variables["ordinal"]]
X_train_numerical = X_train[variables["numerical"]]
X_test_ordinal = X_test[variables["ordinal"]]
X_test_numerical = X_test[variables["numerical"]]

# Remove any rows with missing values for nominal features
X_train_nominal = X_train[variables["nominal"]]
X_train_nominal_missing = np.array(X_train_nominal.isna()).any(axis = 1)
X_train_nominal_missing_indices = np.where(X_train_nominal_missing)[0]
X_train.drop(X_train_nominal_missing_indices, inplace = True)
X_train_ordinal.drop(X_train_nominal_missing_indices, inplace = True)
X_train_numerical.drop(X_train_nominal_missing_indices, inplace = True)
X_train_nominal.drop(X_train_nominal_missing_indices, inplace = True)
Y_train.drop(X_train_nominal_missing_indices, inplace = True)

X_test_nominal = X_test[variables["nominal"]]
X_test_nominal_missing = np.array(X_test_nominal.isna()).any(axis = 1)
X_test_nominal_missing_indices = np.where(X_test_nominal_missing)[0]

X_test.drop(X_test_nominal_missing_indices, inplace = True)
X_test_ordinal.drop(X_test_nominal_missing_indices, inplace = True)
X_test_numerical.drop(X_test_nominal_missing_indices, inplace = True)
X_test_nominal.drop(X_test_nominal_missing_indices, inplace = True)
Y_test.drop(X_test_nominal_missing_indices, inplace = True)

# Combine ordinal, numerical, and nominal sets
X_train = pd.concat((X_train_numerical, X_train_ordinal, X_train_nominal), axis = 1)
X_test = pd.concat((X_test_numerical, X_test_ordinal, X_test_nominal), axis = 1)

# Build data transformation pipeline
steps_ordinal = [("encoder", OrdinalEncoder())]
steps_nominal = [("encoder", OneHotEncoder(drop = "first"))]
steps_numerical = [("imputer", SimpleImputer(strategy = "median")), \
                   ("scaler", StandardScaler())]
pipeline_ordinal = Pipeline(steps_ordinal)
pipeline_nominal = Pipeline(steps_nominal)
pipeline_numerical = Pipeline(steps_numerical)

steps_all = [("ordinal", pipeline_ordinal, variables["ordinal"]),
             ("nominal", pipeline_nominal, variables["nominal"]),
             ("numerical", pipeline_numerical, variables["numerical"])]
pipeline = ColumnTransformer(steps_all)

X_train_tr = pipeline.fit_transform(X_train).toarray()
X_test_tr = pipeline.transform(X_test).toarray()

# Label encoding
label_map = {"PDO.Property Damage Only" : 0,
             "C.Nonvisible Injury"      : 1,
             "B.Visible Injury"         : 2,
             "A.Severe Injury"          : 3,
             "K.Fatal Injury"           : 4}
Y_train_tr = Y_train.replace(label_map)
Y_test_tr = Y_test.replace(label_map)

# Visualize PCA for various numbers of principal components
pca = PCA()
pca.fit(X_train_tr)
pca_variances = np.cumsum(pca.explained_variance_ratio_)
plt.figure()
plt.plot(pca_variances)
plt.xlabel('Number of Components')
plt.ylabel('Variance (%)') # for each component
plt.show()

# Apply PCA to the data
n_pc = np.where(pca_variances >= 0.99)[0][0] + 1
pca = PCA(n_components = n_pc).fit(X_train_tr)
X_train_pca = pca.transform(X_train_tr)
X_test_pca = pca.transform(X_test_tr)

# Number of classes
n_classes = np.unique(Y_train_tr).size
n_classes

model = LogisticRegression()
model.fit(X_train_pca, Y_train_tr)

# Make predictions
Y_train_pred = model.predict(X_train_pca)
Y_test_pred = model.predict(X_test_pca)

# Calculate metrics for training set
acc_train = accuracy_score(Y_train_tr, Y_train_pred)
prec_train = precision_score(Y_train_tr, Y_train_pred, average = None)
recall_train = recall_score(Y_train_tr, Y_train_pred, average = None)
f1_train = f1_score(Y_train_tr, Y_train_pred, average = None)

# Calculate metrics for testing set
acc_test = accuracy_score(Y_test_tr, Y_test_pred)
prec_test = precision_score(Y_test_tr, Y_test_pred, average = None)
recall_test = recall_score(Y_test_tr, Y_test_pred, average = None)
f1_test = f1_score(Y_test_tr, Y_test_pred, average = None)

# Create dataframe
metrics_train = np.array([acc_train, \
                          prec_train[0], prec_train[1], prec_train[2], prec_train[3], prec_train[4], \
                          recall_train[0], recall_train[1], recall_train[2], recall_train[3], recall_train[4], \
                          f1_train[0], f1_train[1], f1_train[2], f1_train[3], f1_train[4]])

metrics_test = np.array([acc_test, \
                         prec_test[0], prec_test[1], prec_test[2], prec_test[3], prec_test[4], \
                         recall_test[0], recall_test[1], recall_test[2], recall_test[3], recall_test[4], \
                         f1_test[0], f1_test[1], f1_test[2], f1_test[3], f1_test[4]])

metrics = {"Training Set": metrics_train, "Testing Set": metrics_test}
metrics = pd.DataFrame(metrics)
metrics.index = ["Accuracy", \
                 "Precision (Class 0)", "Precision (Class 1)", "Precision (Class 2)", "Precision (Class 3)", "Precision (Class 4)", \
                 "Recall (Class 0)", "Recall (Class 1)", "Recall (Class 2)", "Recall (Class 3)", "Recall (Class 4)", \
                 "F1 Score (Class 0)", "F1 Score (Class 1)", "F1 Score (Class 2)", "F1 Score (Class 3)", "F1 Score (Class 4)"]

# Display metrics
metrics